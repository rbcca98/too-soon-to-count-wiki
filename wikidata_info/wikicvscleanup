# -*- coding: utf-8 -*-
"""wikicsvcleanup.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15hJx-kiz6NYwmKdMIFOHGDvsHNyVFxGn
"""

from google.colab import drive
drive.mount('/content/drive')

import csv
import pandas as pd
newwiki = []

with open('/content/drive/My Drive/Wiki II/afd-bios-20170101-2018-01.csv', newline='') as csvfile:
  spamreader = csv.reader(csvfile, delimiter=',')
  for row in spamreader:
    newwiki.append(row[1])
print(newwiki)

import csv
import pandas as pd
newwiki1 = []
irrelevant = []
with open('/content/drive/My Drive/Wiki/FINAL_archive2_wiki - Sheet2.csv', newline='') as csvfile:
  spamreader = csv.reader(csvfile, delimiter=',')
  for row in spamreader:
    if "List of" in str(row[0]):
      irrelevant.append(row)
    #if "2nd nomination" in str(row[0]):
      #irrelevant.append(row)
    #if "3rd nomination" in str(row[0]):
      #irrelevant.append(row)
    else:
      newwiki1.append(row)
      #print(row[0])
#print(irrelevant)
print(newwiki1)

import numpy as np
irrelevant = np.array(irrelevant)
print(irrelevant)

print(newwiki)

newlinks = []
newlinknones = []
for ii in newwiki:
  newlink = ii.replace('/wiki/Wikipedia:Articles_for_deletion/','')
  if '2nd_nomination' in newlink:
    n = newlink.replace('_(2nd_nomination','')
    newlinks.append(n)
    #print(newlink)
  elif '3rd_nomination' in newlink:
    n = newlink.replace('_(3rd_nomination','')
    newlinks.append(n)
  elif '4th_nomination' in newlink:
    n = newlink.replace('_(4th_nomination','')
    newlinks.append(n)
  else:
    newlinks.append(newlink)
print(newlinks)

newerlinks = []
for i in newlinks:
  j = i.replace('_','+')
  k = j.replace('(','')
  print(k)
  ii='https://www.wikidata.org/w/index.php?search='+k
  newerlinks.append(ii)
print(newerlinks)

import pandas as pd
df = pd.DataFrame(q)
df.to_csv('/content/drive/My Drive/Wiki/FINAL_all_wikidata.csv', index=False, header=False)

import requests
from bs4 import BeautifulSoup
first_result_links = []
for ii in newerlinks:
  page = requests.get(ii)
  soup = BeautifulSoup(page.content, 'html.parser')
  results = soup.find(id='mw-content-text')
  elems = results.find_all('li', class_='mw-search-result')
  if len(elems)<1:
    first_result_links.append("no search results")
    print("no search results")
  else:
    first = elems[0]
    heading = first.find('div', class_='mw-search-result-heading')
    link = first.find('a')['href']
    first_result_links.append(link)
    print(link)

linksfinal = []
for i in first_result_links:
  if 'no search results' in i:
    linksfinal.append('no search results')
  else:
    j = 'https://www.wikidata.org'+i
    linksfinal.append(j)
print(linksfinal)

genders = []
for i in linksfinal:
  if 'no search results' in i:
    genders.append('no search results')
    print('no search results')
  else:
    page = requests.get(i)
    soup = BeautifulSoup(page.content, 'html.parser')
    results = soup.find(id='P21')
    #print(str(type(results)))
    if str(type((results)))=="<class 'NoneType'>":
      genders.append('gender unknown')
      print('gender unknown')
    else:
      elems = results.find('div', class_='wikibase-snakview-value wikibase-snakview-variation-valuesnak')
      genders.append(elems.text.strip())
      print(elems.text.strip())

occ = []
for i in linksfinal:
  if 'no search results' in i:
    occ.append('no search results')
    print('no search results')
  else:
    page = requests.get(i)
    soup = BeautifulSoup(page.content, 'html.parser')
    results = soup.find(id='P106')
    #print(str(type(results)))
    if str(type((results)))=="<class 'NoneType'>":
      occ.append('occupation unknown')
      print('occupation unknown')
    else:
      elems = results.find('div', class_='wikibase-snakview-value wikibase-snakview-variation-valuesnak')
      occ.append(elems.text.strip())
      print(elems.text.strip())

emp = []
for i in linksfinal:
  if 'no search results' in i:
    emp.append('no search results')
    print('no search results')
  else:
    page = requests.get(i)
    soup = BeautifulSoup(page.content, 'html.parser')
    results = soup.find(id='P108')
    #print(str(type(results)))
    if str(type((results)))=="<class 'NoneType'>":
      emp.append('employer unknown')
      print('employer unknown')
    else:
      elems = results.find('div', class_='wikibase-snakview-value wikibase-snakview-variation-valuesnak')
      emp.append(elems.text.strip())
      print(elems.text.strip())

import requests
from bs4 import BeautifulSoup
descriptions = []
for i in linksfinal:
  if 'no search results' in i:
    descriptions.append('no search results')
    print('no search results')
  else:
    page = requests.get(i)
    soup = BeautifulSoup(page.content, 'html.parser')
    results = soup.find(id='mw-content-text')
    #print(str(type(results)))
    if str(type((results)))=="<class 'NoneType'>":
      descriptions.append('no description')
      print('no description')
    else:
      elems = results.find('span', class_='wikibase-descriptionview-text')
      descriptions.append(elems.text.strip())
      print(elems.text.strip())

a = np.array(newerlinks)
b = np.array(linksfinal)
c = np.array(genders)
a = a.T
b = b.T
c = c.T
d = np.array((a,b,c))
d = d.T
f = np.array(d)
g = np.array(newwiki1)
e = np.concatenate((g,f),axis=1)
print(e)

descriptionslist = list(descriptions)
zeroslist = list(zeros([2320,1]))
o= np.array((descriptionslist,zeroslist))

import numpy as np
p = o.T
#m = np.append(newwiki,descriptionslist,axis=1)
n = np.concatenate((newwiki, p),axis=1)
q = np.delete(n,10,axis=1)
print(q)

occ = np.array(occ)
emp = np.array(emp)
occ = occ.T
emp = emp.T
h = np.array((occ,emp))
h = h.T
i = np.array(h)
l = np.concatenate((e,i),axis=1)
print(l)
