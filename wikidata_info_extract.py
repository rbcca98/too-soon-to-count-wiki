# -*- coding: utf-8 -*-
"""Wikidata_Info_Extract.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PUwQKYvGye8TF5muSpf02erg02PO_lFZ
"""

from google.colab import drive
drive.mount('/content/drive')

"""Getting Gender data from Wiki Metadata
- Make sure to correctly enter the path to your file
"""

#Enter the path to your csv file
path_to_file='/content/drive/My Drive/4sen2.0/Original Data/afd-bios-20170101-2018-01_done.csv'
import csv
import pandas as pd
newwiki = []

with open(path_to_file, newline='') as csvfile:
  spamreader = csv.reader(csvfile, delimiter=',')
  for row in spamreader:
    newwiki.append(row[3])
print(newwiki)

newwiki1=[]
for ii in newwiki:
  if '(' in ii:
    n=ii.replace('(','')
    m=n.replace(')','')
    newwiki1.append(m)
  else:
    newwiki1.append(ii)
print(newwiki1)
newwiki2=[]
for jj in newwiki1:
  p=jj.replace('https://en.wikipedia.org/wiki/Wikipedia:Articles_for_deletion/','')
  o=p.replace('_','+')
  q=o.replace('+2nd+nomination','')
  r= q.replace('+3rd+nomination','')
  newwiki2.append(r)
print(newwiki2)

newwiki3=[]
for kk in newwiki2:
  s='https://www.wikidata.org/w/index.php?search='+kk
  newwiki3.append(s)
print(newwiki3)

import requests
from bs4 import BeautifulSoup
first_result_links = []
for ii in newwiki3:
  page = requests.get(ii)
  soup = BeautifulSoup(page.content, 'html.parser')
  results = soup.find(id='mw-content-text')
  elems = results.find_all('li', class_='mw-search-result')
  if len(elems)<1:
    first_result_links.append("no search results")
    print("no search results")
  else:
    first = elems[0]
    heading = first.find('div', class_='mw-search-result-heading')
    link = first.find('a')['href']
    first_result_links.append(link)
    print(link)

result_links=[]
for ii in first_result_links:
  if 'no search results' in ii:
    result_links.append('no search results')
  else:
    q='https://www.wikidata.org'+ii
    result_links.append(q)
print(result_links)

for i in result_links:
  print(i)

genders = []
for i in result_links:
  if 'no search results' in i:
    genders.append('no search results')
    print('no search results')
  else:
    page = requests.get(i)
    soup = BeautifulSoup(page.content, 'html.parser')
    results = soup.find(id='P21')
    #print(str(type(results)))
    if str(type((results)))=="<class 'NoneType'>":
      genders.append('gender unknown')
      print('gender unknown')
    else:
      elems = results.find('div', class_='wikibase-snakview-value wikibase-snakview-variation-valuesnak')
      if elems:
        genders.append(elems.text.strip())
        print(elems.text.strip())
      else:
        genders.append('gender unknown')
        print('gender unknown')

"""Getting Consensus from AfD Link"""

consensus = []
import requests
from bs4 import BeautifulSoup
for i in newwiki:
  if 'AfD Link' in i:
    consensus.append('AfD Link')
    print('AfD Link')
  else:
    if not i:
      consensus.append('none')
      print('none')
    else:
      page = requests.get(i)
      soup = BeautifulSoup(page.content, 'html.parser')
      results = soup.find(id='mw-content-text')
      #print(str(type(results)))
      if str(type((results)))=="<class 'NoneType'>":
        consensus.append('no description')
        print('no description')
      else:
        elems = results.find_next(class_='boilerplate afd vfd xfd-closed')
        if not elems:
          consensus.append('none')
          print('none')
        else:
          elems1=elems.find_next('p')
          elems2=elems1.find_next('b')
          consensus.append(elems2.text.strip())
          print(elems2.text.strip())

print(len(result_links), len(genders), len(consensus))

dat = pd.DataFrame({"Link":result_links, "Gender":genders, "Consensus":consensus})
print(len(result_links), len(genders), len(consensus))
dat.drop(index=0, inplace=True)
dat.index = range(len(result_links)-1)
dat.head(5)

import gspread
temp = pd.read_csv(path_to_file)
temp.tail(5)

temp = temp.join(dat)
temp.to_csv(path_to_file[:-4]+"_done.csv")
temp.head(10)

# filtering the data
import csv
import pandas as pd
path_to_file='/content/drive/My Drive/4sen2.0/Original Data/afd-bios-20170101-2020-03_done.csv'

orig = pd.read_csv(path_to_file)
for index, row in orig.iterrows():
  temp = row["Consensus"].lower()
  nme = row["Entry"].lower()
  if "list" in nme or "team" in nme or "band" in nme:
    print("[Name]", index, nme)
  if "keep" in temp or "withdrawn" in temp:
    orig.iloc[index, 9] = "Keep"
  elif "delete" in temp:
    orig.iloc[index, 9] = "Delete"
  elif "merge" in temp or "redirect" in temp:
    orig.iloc[index, 9] = "Merge/Redirect"
  elif "no consensus" in temp:
    orig.iloc[index, 9] = "No Consensus"
  else:
    print(index, nme, temp, row["Page Link"])
    newt = input("prompt: ")
    orig.iloc[index, 9] = newt

sh = gc.create(path_to_file[:-9])
ws = sh.get_worksheet(0)
set_with_dataframe(ws, orig)

from google.colab import auth
auth.authenticate_user()

import gspread
from oauth2client.client import GoogleCredentials

gc = gspread.authorize(GoogleCredentials.get_application_default())

from gspread_dataframe import set_with_dataframe
#orig.to_csv(path_to_file[:-4]+"_done.csv")