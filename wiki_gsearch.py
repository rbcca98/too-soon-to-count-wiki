# -*- coding: utf-8 -*-
"""wiki_gsearch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EndqA9Sq2Oz5JnJQ_QpGZm7x2sQ6yNC0
"""

from google.colab import drive
drive.mount('/content/drive')
import csv
import pandas as pd
names = []

with open('/content/drive/My Drive/Wiki/gsearchlinks.csv', newline='') as csvfile:
  spamreader = csv.reader(csvfile, delimiter=',')
  for row in spamreader:
    names.append(row[0])
!pip install selenium
!apt-get update # to update ubuntu to correctly run apt install
!apt install chromium-chromedriver
!cp /usr/lib/chromium-browser/chromedriver /usr/bin
import sys
sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')
from selenium import webdriver
chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')
wd = webdriver.Chrome('chromedriver',chrome_options=chrome_options)

occ=[]
import requests
from bs4 import BeautifulSoup
for i in names[2150:2320]:
  if 'N/A' in i:
    occ.append('N/A')
    print('N/A')
  elif 'Google Search Link' in i:
    occ.append('Google Result Number')
    print('Google Result Number')
  else:
    #driver = webdriver.Chrome(executable_path='C:/path/to/chromedriver.exe')
    #wd = webdriver.Chrome('chromedriver',options=options)
    wd.get(i)
    #page = requests.get(i)
    soup = BeautifulSoup(wd.page_source, 'html.parser')
    results = soup.find(id='result-stats')
    #print(str(type(results)))
    if str(type((results)))=="<class 'NoneType'>":
      occ.append('error')
      print('error')
    else:
      #elems = results.find('div', class_='wikibase-snakview-value wikibase-snakview-variation-valuesnak')
      occ.append(results.text.strip())
      print(results.text.strip())